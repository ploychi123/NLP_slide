Introduction 5%
- อธิบายงานทาง NLP โดยยกตัวอย่าง input และ output ที่ได้ของงาน
Text Classification คือ การจัดแบ่งประเภทของเอกสาร เช่น อีเมล์ รีวิวสินค้า ข้อความในแชท ให้อยู่ในหมวดหมู่ที่กำหนด โดยหนึ่งเอกสารอาจจะอยู่มากกว่าหนึ่งหมวดก็ได้ ตัวอย่างหมวดหมู่เช่น อีเมล์ดี หรือ อีเมล์ขยะ, คะแนนรีวิวดี กลาง ต่ำ แย่, ภาษาในแชทนั้นคือภาษาอะไร การทำ Text Classification สามารถใช้หลากหลายวิธีการ วิธีที่นิยมในปัจจุบันจะใช้ Machine Learning





---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
nltk & text corpus 10%
- อธิบายศัพท์ทาง NLP และยกตัวอย่างโค้ด
1.Tokenization (การตัดคำ): การแบ่งข้อความออกเป็นหน่วยย่อยที่เรียกว่า "โทเค็น" ซึ่งอาจเป็นคำ สัญลักษณ์ หรือวรรค ซึ่งเป็นขั้นตอนสำคัญในการประมวลผลภาษาธรรมชาติ

# ตัวอย่างโค้ดการตัดคำ (Tokenization) ด้วย NLTK
from nltk.tokenize import word_tokenize

text = "Natural Language Processing is fascinating."
tokens = word_tokenize(text)
print(tokens)
# Output: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '.']


2.Stemming (การเปลี่ยนรูปคำ): กระบวนการลบเครื่องหมายที่ไม่จำเป็นจากคำ เพื่อให้คำที่มีความหมายเดียวกันแต่เป็นรูปแบบต่างๆ มีรูปแบบเดียวกัน
3.Lemmatization (การหารูปฟอร์มหลักของคำ): กระบวนการแปลงคำให้เป็นรูปที่มีความหมายเดียวกันในรูปแบบที่มีความเป็นไปได้ที่สุด เช่น "went" จะถูกแปลงเป็น "go"
4.Part-of-speech (POS) Tagging (การแท็กส่วนที่ของคำ): กระบวนการกำหนดประเภทของคำในประโยค เช่น กริยา นาม สรรพนาม เป็นต้น
5.N-Gram คือ หนึ่งในโมเดล NLP ที่นิยมใช้กันมากที่สุด N-Gram คือ ลำดับของคำต่อเนื่องกัน จำนวน N คำ จากชุดข้อความ เพื่อนำมาเป็นข้อมูลสถิติ เช่น วิเคราะห์ ความถี่ ความสัมพันธ์ระหว่างคำ โอกาสความน่าจะเป็น และพยากรณ์คำต่อไป
6.Bag of Words (BOW)
โมเดลที่ใช้กันแพร่หลายในงานจัดแบ่งประเภทข้อความ Text Classification ในโมเดลของ BOW กลุ่มของคำจะถูกอธิบายด้วยกระเป๋าคำ Bag of words หรือกลุ่มรวมของคำ โดยไม่ได้คำนึงถึงหลักไวยากรณ์ ความถี่ที่พบ และลำดับของคำ โดยนำมาใช้เป็น Feature ในการเทรนตัวจัดแบ่งข้อความ Classifier

https://www.bualabs.com/archives/119/what-is-nlp-natural-language-processing-nlp-task-in-thai-nlp-ep-1/









--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
web scraping 5%
- ดึงข้อมูลจากเว็บที่ระบุ โดยเลือกข้อมูลอย่างน้อย 3 ข้อมูล จัดเก็บในรูปแบบ csv


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
word representation 5%
- ตอบคำถามเกี่ยวกับการแทนคำในรูปแบบต่างๆเขียนโปรแกรมเกี่ยวกับ word representation
One-hot Encoding:
One-hot encoding เป็นการแทนคำด้วยเวกเตอร์ที่มีความยาวเท่ากับจำนวนคำทั้งหมดในโปรแกรม. โดยมีค่า 1 หรือ 0 บนแต่ละตำแหน่งตามลำดับของคำที่แตกต่างกัน
ในที่นี้ เรามีคำว่า "I", "love", "natural", "language", "processing" ดังนั้นจำนวนคำทั้งหมดคือ 5
ต่อไปเราจะทำการแทนคำแต่ละคำด้วย One-hot encoding โดยเริ่มต้นจากการกำหนดตำแหน่งของแต่ละคำในลิสต์ จากนั้นกำหนดค่า 1 ที่ตำแหน่งนั้นและค่า 0 ที่ตำแหน่งอื่นๆ

sentence = "I love natural language processing"
words = sentence.split()
word_to_index = {word: i for i, word in enumerate(words)}
print("Word to index mapping:", word_to_index)

# One-hot encoding
one_hot_encoding = []
for word in words:
    one_hot_vector = [0] * len(words)
    one_hot_vector[word_to_index[word]] = 1
    one_hot_encoding.append(one_hot_vector)

print("One-hot encoding:")
for word, encoding in zip(words, one_hot_encoding):
    print(word, ":", encoding)

Word Embeddings:
Word embeddings เป็นเทคนิคการแทนคำในรูปแบบของเวกเตอร์ที่มีมิติน้อยกว่า one-hot encoding และเก็บข้อมูลความสัมพันธ์ระหว่างคำได้ดีขึ้น
ในที่นี้ โดยใช้โมเดล word2vec ในการสร้าง word embeddings ของคำแต่ละคำในประโยค

from gensim.models import Word2Vec

# Split sentence into words
words = sentence.split()

# Train word2vec model
model = Word2Vec([words], min_count=1, vector_size=5)

# Get word embeddings
word_embeddings = {}
for word in words:
    word_embeddings[word] = model.wv[word]

print("Word embeddings:")
for word, embedding in word_embeddings.items():
    print(word, ":", embedding)

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
text classification & logistic regression & neural network 10%
ตอบคำถาม และอธิบายโค้ดประกอบเขียนโปรแกรมเพื่อจำแนกประเภทอย่างง่าย

1. อธิบายวิธีการใช้ Logistic Regression ในการจำแนกประเภทของข้อมูล:

Logistic Regression เป็นวิธีการจำแนกประเภทของข้อมูลที่ใช้เพื่อคาดการณ์ความน่าจะเป็นของการเป็นสมาชิกในกลุ่มหนึ่งๆ โดยใช้ฟังก์ชัน Logistic เพื่อแปลงผลลัพธ์เป็นค่าความน่าจะเป็นระหว่าง 0 ถึง 1 ซึ่งบ่งบอกถึงการเป็นสมาชิกของกลุ่มนั้นๆ

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# ข้อมูลตัวอย่าง
X = [[1, 2], [2, 3], [3, 4], [4, 5]]
y = [0, 0, 1, 1]

# แบ่งข้อมูลเป็นชุดฝึกและชุดทดสอบ
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# สร้างโมเดล Logistic Regression
model = LogisticRegression()

# เทรนโมเดล
model.fit(X_train, y_train)

# ทำนายผลลัพธ์
y_pred = model.predict(X_test)

# คำนวณความแม่นยำ
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# แสดงรายงานการจำแนกประเภท
print("Classification Report:")
print(classification_report(y_test, y_pred))

3. อธิบายวิธีการใช้ Neural Network ในการจำแนกประเภทของข้อมูล:

Neural Network เป็นวิธีการจำแนกประเภทของข้อมูลที่ใช้เรียนรู้โดยมีโครงสร้างของโครงข่ายประสาทเทียม ที่ประกอบด้วยชั้น Input, Hidden และ Output โดยสามารถใช้การเรียนรู้เชิงลึก (Deep Learning) เพื่อหาความสัมพันธ์ซับซ้อนในข้อมูล

import numpy as np
from keras.models import Sequential
from keras.layers import Dense

# ข้อมูลตัวอย่าง
X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
y = np.array([0, 0, 1, 1])

# สร้างโมเดล Neural Network
model = Sequential()
model.add(Dense(units=2, activation='relu', input_dim=2))  # ชั้น Input
model.add



